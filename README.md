# BDCI-2017：基于机构实体的智能摘要和风险等级识别 [![experimental](http://badges.github.io/stability-badges/dist/experimental.svg)](http://github.com/badges/stability-badges)
> 本队队名是“+1s”。初赛成绩为40/230，复赛成绩是A榜13/230，B榜17/230
## 题目回顾
### 任务描述
**对数据集中的每条记录，提取出正文中的主要机构实体，并生成智能摘要。**</br>
风险等级分三类：正向、负向、中性；疑似经营风险、虚假宣传、司法风险等都判定为负向；新产品问世、通过质量认证等都判定为正向；可参考数据集中的正负面词汇。</br>
*注：同一篇新闻中，不同的机构实体，对应的关键词、风险等级和摘要可能不同。*

### 数据集

#### 文件清单和使用说明

1. [DATA_T.txt]()：训练数据集
2. [DATA_T_RESULT.txt]()：训练数据集参考结果
3. [NEGATIVE_WORDS.xls]()：正负面词汇供参考
4. [DATA_TEST.txt]()：测试数据集

#### 数据来源和背景

法海风控采集的司法、财经、科技、质量监督类，由权威媒体发布的新闻舆情数据；从原始网页中提取了标题、正文、发布时间等内容，组织成JSON格式。

#### 文件格式

文本文件，每行一条记录，\r\n 为换行符；</br>
每行为一个JSON格式数据，详细字段定义见下表。

#### 数据格式

|字段|类型|名称|备注|
|:--|:--|:--|:--|
|newsId|Text|标识ID||
|srcUrl|text|来源链接|部分链接可能失效|
|title|text|标题||
|postTime|long|发布时间|时间戳-毫秒|
|body|text|正文|包含标签|

### 评分方法

按 “准确度\*系数” 的方式来评分，满分100。 </br>
得分=（用户负向/标准负向）\*50+（用户正向/标准正向）\*30+（用户中性/标准中性）\*20 </br>
系数= 用户识别实体数量/标准实体数量 （小于1时为1，超出1.3倍以后，数量越大，值越小） 

## 过程简述
### 预处理
#### 1.数据清洗
我们利用[json-csv转换器](https://json-csv.com/)，将[DATA_T.txt]()和[DATA_T_RESULT.txt]()里面的json数据转入[TRAIN_SET.xlsx]()，方便查看；同时，我们将其中的全部实体提出，即[train.ne]()
#### 2.分词与词性标注
我们利用[Stanford CoreNLP]()工具包，针对[train.ne]()中的数据进行了分词与词性标注。在分析结果文件[train_ne_seg_pos.file]()时，我们发现政府机构与企业机构均具有一定的常见模式，且该模式具有85%以上的覆盖率：

> 模式

之后，我们利用[wordcount工具包]()，对[train_ne_seg_pos.file]()中的分词词频进行了统计与排序，见[wordfreq.xlsx]()。我们从频次大于100的后缀词中选取了匹配键，进行匹配。

### 机构实体识别A:基于成词模式

根据前一步获得的模式，我们得到一个识别机构实体的模块，[pattern2ne]()。该模块通过扫描[train_ne_seg_pos.file]()，寻找满足【模式】的字段。在这个过程里，我们发现会出现如下的情况：

> 不同实体黏在一起

我们通过对存在于备选实体内部的后缀词进行切割，去除了大部分这种“粘连实体”。除此之外，我们还注意到了大量这样的实体：

> 过短的实体（缺头）

这是因为部分实体的结构没有完全符合我们定义的成词模式，例如“”。这种错误的出现主要是在于【】。针对这些错误实体，我们通过判断去除后缀词后的剩余长度的方法，对此类实体进行剔除，取得了较好的效果。</br>
最后，我们对获得的实体长度进行了限制，得到了第一种机构实体识别结果[pattern.ne]()

### 机构实体识别B:基于NER处理

我们仍然采用[Stanford CoreNLP]()工具包。首先，我们将[DATA_TEST.txt]()中的全部body(新闻正文)提取出来，并在每条新闻前加入“北航”二字，以方便在命名实体识别(NER)后分离不同新闻。在这种情况下，我们使用工具包的NER功能，将标注为organzition的实体存入[direct.ne]()

### 机构实体识别C:基于结构化数据提取

通过对数据清洗得到的表格中的body域的审阅，我们注意到，大量的新闻中包含有表格，而在“中国质量新闻网”等来源的新闻表格中，往往是对抽查、质检等结果的公式，有着极高的实体提取和情感分析的价值。于是，我们完成了从半结构化的新闻中抽取这类结构化表格的模块[table2ne]。该模块通过判定每条新闻中的表格，将各个表格写入一个三维列表，实现了对表格的重建。接下来，我们对表格中的每一格进行扫描，对照【】，如果发现某一格中的内容恰为之前已经找到过的实体，则判定其所在的列均为实体，将其写入[table.ne]()，以此实现对实体集合的扩大。</br>
*这种方法得到的实体有着极高的准确性*

### 情感判定
(在各来源的机构实体识别过程中，我们其实分别完成了情感的判定。此处主要是陈述具体的操作过程)</br>
通过对训练集的观察，我们发现，几乎全部的“政府”类实体均为中向。因此，我们首先依据后缀特征词，对得到的全部实体进行二分类。</br>
对于来源是非结构化文本的“非政府”实体，我们在赛事官方数据的基础上，通过修改，得到了一个自己的情感词词典[sentiment.dict]()。通过裁剪每个实体所在句段的情感词，判断该实体情感。同时，考虑到训练集中的负向情感较多，且积分方法里负向情感权重较大，因此实际评分算法较为倾向负向；对于来源是结构化数据的“非政府”实体，我们在同一行寻找情感词判断，如果找不到则在表头里寻找，如果仍没有则置为负向；对于“政府”实体，一律置为中向。

### 数据整合
在本方法里，情感判定是基于实体抽取的结果进行的。在实际操作里，我们发现基于NER直接获得的机构实体识别B质量最差。考虑到机构实体识别C的生成过程中已经吸收了前者的信息，同时实体过量会导致得分降低（通过在A榜阶段“试榜”，我们估计最优实体数量约为21100条），我们决定在最终集成时只输入机构实体识别A与C得到的结果以及它们对应的情感信息。同时，在输入之前，我们分别对两个文件进行了去重处理，即把[unique_pattern.nesen]()和[unique_table.nesen]()整合为了符合评测要求的[fin.txt]()。

## 迭代过程
:one:直接采用Stanford工具包进行命名实体识别，全部情感置为负向。**约17分**</br>
:two:采用Stanford工具包与后缀匹配进行联合实体识别，二分类后判定情感。**约21分**</br>
:three:最新版本。采用表格抽取与后缀匹配进行联合实体识别，通过情感判定模块进行更细粒度的情感判定。**约36分**
## 复现方法及具体代码说明
请参考对应模块下的说明及代码内注释。</br>
如果有任何问题或建议，欢迎通过issue提出。
