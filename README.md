# BDCI2017-基于机构实体的智能摘要和风险等级识别 [![experimental](http://badges.github.io/stability-badges/dist/experimental.svg)](http://github.com/badges/stability-badges)
> 本队的初赛成绩为18/230，复赛成绩是
## 题目回顾
### 任务描述
**对数据集中的每条记录，提取出正文中的主要机构实体，并生成智能摘要。**</br>
风险等级分三类：正向、负向、中性；疑似经营风险、虚假宣传、司法风险等都判定为负向；新产品问世、通过质量认证等都判定为正向；可参考数据集中的正负面词汇。</br>
注：同一篇新闻中，不同的机构实体，对应的关键词、风险等级和摘要可能不同。
### 数据集
#### 文件清单和使用说明
1. [DATA_T.txt]()：训练数据集
2. [DATA_T_RESULT.txt]()：训练数据集参考结果
3. [NEGATIVE_WORDS.xls]()：正负面词汇供参考
4. [DATA_TEST.txt]()：测试数据集
#### 数据来源和背景
法海风控采集的司法、财经、科技、质量监督类，由权威媒体发布的新闻舆情数据；从原始网页中提取了标题、正文、发布时间等内容，组织成JSON格式。
#### 文件格式
文本文件，每行一条记录，\r\n 为换行符；
每行为一个JSON格式数据，详细字段定义见下表。
#### 数据格式

|字段|类型|名称|备注|
|:--|:--|:--|:--|
|newsId|Text|标识ID||
|srcUrl|text|来源链接|部分链接可能失效|
|title|text|标题||
|postTime|long|发布时间|时间戳-毫秒|
|body|text|正文|包含标签|

## 过程简述
### 预处理
#### 1.数据清洗
- 我们利用【】，将[DATA_T.txt]()和[DATA_T_RESULT.txt]()里面的json数据转入[TRAIN_SET.xlsx]()，方便查看；同时，我们将其中的全部实体提出，即[train.ne]()
#### 2.分词与词性标注
我们利用[Stanford CoreNLP]()工具包，针对[train.ne]()中的数据进行了分词与词性标注。在分析结果文件[train_ne_seg_pos.file]()时，我们发现政府机构与企业机构均具有一定的常见模式，且该模式具有85%以上的覆盖率：

> 模式

之后，我们利用[wordcount工具包]()，对[train_ne_seg_pos.file]()中的分词词频进行了统计与排序，见[wordfreq.xlsx]()。我们从中选出了TOP 0的后缀词作为匹配键，进行匹配。

### 机构实体识别A:基于成词模式

根据前一步获得的模式，我们得到一个识别机构实体的模块，[pattern2ne]()。该模块通过扫描[train_ne_seg_pos.file]()，寻找满足【】的字段。在这个过程里，我们发现会出现如下的情况：

> 不同实体黏在一起

我们通过对存在于备选实体内部的后缀词进行切割，去除了大部分这种“粘连实体”。除此之外，我们还注意到了大量这样的实体：

> 过短的实体（缺头）

这是因为部分实体的结构没有完全符合我们定义的成词模式，例如“ ”，它【为什么不符合】。针对这些错误实体，我们通过判断去除后缀词后的剩余长度的方法，对此类实体进行剔除，取得了较好的效果。
最后，我们对获得的实体长度进行了限制，得到了第一种机构实体识别结果[pattern.ne]()

### 机构实体识别B:基于NER处理

我们仍然采用[Stanford CoreNLP]()工具包。首先，我们将[DATA_TEST.txt]()中的全部body(新闻正文)提取出来，并在每条新闻前加入“北航”二字，以方便在命名实体识别(NER)后分离不同新闻。在这种情况下，我们使用工具包的NER功能，将标注为organzition的实体存入[direct.ne]()

### 机构实体识别C:基于结构化数据提取

通过对数据清洗得到的表格中的body域的审阅，我们注意到，大量的新闻中包含有表格，而在“中国质量新闻网”等来源的新闻表格中，往往是对抽查、质检等结果的公式，有着极高的实体提取和情感分析的价值。于是，我们完成了从半结构化的新闻中抽取这类结构化表格的模块[table2ne]。该模块通过判定每条新闻中的表格，将各个表格写入一个三维列表，实现了对表格的重建。接下来，我们对表格中的每一格进行扫描，对照【】，如果发现某一格

### 情感分析

### 数据整合

## 迭代过程

## 复现方法及具体代码说明
请参考对应模块下的说明及代码内注释。</br>
如果有任何问题或建议，欢迎通过issue提出。
